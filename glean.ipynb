{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"train.csv\"\n",
    "test = \"test.csv\"\n",
    "labels = \"labels.csv\"\n",
    "\n",
    "train_set = pd.read_csv(train,encoding='unicode_escape')\n",
    "test_set = pd.read_csv(test,encoding='unicode_escape')\n",
    "label_set = pd.read_csv(labels,encoding='unicode_escape')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_words = list()\n",
    "train_set = train_set.replace(np.nan, '', regex=True)\n",
    "item_words += list(chain(*train_set['line_item_name'].str.replace('[^\\w\\s]','').str.lower().str.split().tolist())) # line items\n",
    "item_words += list(chain(*train_set['line_item_description'].str.replace('[^\\w\\s]','').str.lower().str.split().tolist())) # description\n",
    "item_words += list(chain(*label_set['canonical_line_item_name'].str.replace('[^\\w\\s]','').str.lower().str.split().tolist()))  # labels\n",
    "\n",
    "words_counter = Counter(item_words)\n",
    "\n",
    "item_words = list(set(item_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observation of the data:\n",
    "1. Many raw line items are at least partially the same as canonical line item\n",
    "2. The hard cases are those lines where raw line items are substantially different from the canonical line item.\n",
    "\n",
    "Basic idea:\n",
    "1. The simple cases can be easily addressed with similarity comparisons, such as looking at cosine similarity\n",
    "2. Harder cases may require additional information, such as vendor name. A vendor name only maps to a finite number of canonical line items.\n",
    "3. Use cosine similarity to filter out the easy case first, then build a binary classifier for the rest (given raw line item and canonical line item pair, predict if they are from a same line item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector for each line_item_name + description, as well as canonical_line_item_name\n",
    "\n",
    "def line2vec(lineItem,vocab=item_words,description=None):\n",
    "    # lineItem is a list of words in line_item\n",
    "    nVocab = len(vocab)\n",
    "    if description:\n",
    "        lineItem = lineItem+description\n",
    "    word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "    lineVec = np.zeros((1,nVocab+1))\n",
    "    for w in lineItem:\n",
    "        if w in word2idx:\n",
    "            lineVec[0,word2idx[w]] = 1\n",
    "        else:\n",
    "            lineVec[0,-1] = 1\n",
    "    return lineVec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the vector representation of labels, construct as a matrix of dimension n_labs x k_dim\n",
    "# then for each line item input, compute the cosine similarities and find the max\n",
    "\n",
    "all_labs = label_set['canonical_line_item_name'].tolist()\n",
    "lab2idx = {}\n",
    "nlabs = len(all_labs)\n",
    "lab_mat = np.zeros((nlabs,len(item_words)+1))\n",
    "for j,l in enumerate(all_labs):\n",
    "    lab2idx[l] = j \n",
    "    l = re.sub('[^\\w\\s]','',l).lower().split()\n",
    "    lineVec = line2vec(l,vocab=item_words,description=None)\n",
    "    lab_mat[j,:] = lineVec\n",
    "#lab_mat = sparse.csr_matrix(lab_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinVal(lineVec,lab_mat):\n",
    "    outVec = np.zeros(lab_mat.shape[0])\n",
    "    for i in range(lab_mat.shape[0]):\n",
    "        similarity = cosine_similarity(lineVec,lab_mat[None,i])\n",
    "        outVec[i] = similarity\n",
    "    return np.max(outVec), np.argsort(outVec)[::-1][:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One good thing to do before moving forward is to build a dictionary between vendor and canonical line item\n",
    "# because many vendors only have limited number of canonical line item variations\n",
    "\n",
    "# feature space: words in line_item and vendor names\n",
    "vendor2idx = {}\n",
    "vendors = list(set(label_set['canonical_vendor_name']))\n",
    "for j,v in enumerate(vendors):\n",
    "    vendor2idx[v] = j\n",
    "\n",
    "nlabs = len(label_set.index)\n",
    "vendor2item = defaultdict(list)\n",
    "for k in range(nlabs):\n",
    "    vendor = label_set['canonical_vendor_name'][k]\n",
    "    vendor2item[vendor].append(label_set['canonical_line_item_name'][k])\n",
    "    \n",
    "# then in training classifiers below, examples can be constructed only from the possible mappings\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the training set, for each row, compute the cosine similarity between all possible labels\n",
    "# then decide take the most similar label, or pass down for the classifier with a pre-defined threshold\n",
    "# construct positive and negative examples based on the true pair and possible pair based on similarity score\n",
    "\n",
    "threshold = 0.4\n",
    "for_classify = []\n",
    "predicted = []\n",
    "nrows = len(train_set.index)\n",
    "for j in range(nrows):\n",
    "    lineItem = re.sub('[^\\w\\s]','',train_set['line_item_name'][j]).lower().split()\n",
    "    lineDescrpt = re.sub('[^\\w\\s]','',train_set['line_item_name'][j]).lower().split()\n",
    "    lineVec = sparse.csr_matrix(line2vec(line,vocab=item_words,description=lineDescrpt))\n",
    "    vendor = train_set['canonical_vendor_name'][j]\n",
    "    possible_labs = vendor2item[vendor]\n",
    "    labIds = [lab2idx[lab] for lab in possible_labs]\n",
    "    subMat2matId = {i:lab2idx[lab] for i,lab in enumerate(possible_labs)}\n",
    "    subMat = lab_mat[labIds,:]\n",
    "    maxVal, maxIdx = cosinVal(lineVec,subMat)\n",
    "    if maxVal<threshold:\n",
    "        for_classify.append((j,[subMat2matId[k] for k in maxIdx[:5]])) # choose the first 10, see what happens\n",
    "    else:\n",
    "        predicted.append((j,subMat2matId[maxIdx[0]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate based on cosine only: 0.058\n"
     ]
    }
   ],
   "source": [
    "# By just looking at cosine similarity can give us a decent performace\n",
    "# Then I will use the miss-classified items from here and the remaining items to train a classifier\n",
    "\n",
    "wrong = []\n",
    "wrong_item = []\n",
    "for a,b in predicted:\n",
    "    if train_set['canonical_line_item_name'][a] != label_set['canonical_line_item_name'][b]:\n",
    "        wrong.append(a)\n",
    "        wrong_item.append(label_set['canonical_line_item_name'][b])\n",
    "er = len(wrong)/len(predicted)\n",
    "print(\"Error rate based on cosine only: %.3f\" % er)\n",
    "\n",
    "for i in wrong:\n",
    "    lineItem = re.sub('[^\\w\\s]','',train_set['line_item_name'][i]).lower().split()\n",
    "    lineDescrpt = re.sub('[^\\w\\s]','',train_set['line_item_name'][i]).lower().split()\n",
    "    lineVec = sparse.csr_matrix(line2vec(line,vocab=item_words,description=lineDescrpt))\n",
    "    vendor = train_set['canonical_vendor_name'][j]\n",
    "    possible_labs = vendor2item[vendor]\n",
    "    labIds = [lab2idx[lab] for lab in possible_labs]\n",
    "    subMat2matId = {i:lab2idx[lab] for i,lab in enumerate(possible_labs)}\n",
    "    subMat = lab_mat[labIds,:]\n",
    "    maxVal, maxIdx = cosinVal(lineVec,subMat)\n",
    "    for_classify.append((j,[subMat2matId[k] for k in maxIdx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1383, 5170)\n"
     ]
    }
   ],
   "source": [
    "# then collect the items in for_classify to train a binary classifier\n",
    "import pdb\n",
    "\n",
    "label2idx = {}\n",
    "nlabs = len(label_set.index)\n",
    "for j in range(nlabs):\n",
    "    label2idx[label_set['canonical_line_item_name'][j]] = j\n",
    "idx2label = {label2idx[lab]:lab for lab in label2idx}\n",
    "    \n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "not_in_five = []\n",
    "\n",
    "# build examples\n",
    "for j,ids in for_classify:\n",
    "    vendorVec = np.zeros(len(vendor2idx))\n",
    "    #vendor = train_set['canonical_vendor_name'][j]\n",
    "    #vendorVec[vendor2idx[vendor]] = 1\n",
    "    #vendorVec = sparse.csr_matrix(vendorVec)\n",
    "    trueLab = train_set['canonical_line_item_name'][j]\n",
    "    lineItem = re.sub('[^\\w\\s]','',train_set['line_item_name'][j]).lower().split()\n",
    "    lineDescrpt = re.sub('[^\\w\\s]','',train_set['line_item_name'][j]).lower().split()\n",
    "    lineVec = line2vec(line,vocab=item_words,description=lineDescrpt)\n",
    "    curr_y = 0\n",
    "    for i in ids:\n",
    "        labVec = lab_mat[i,:]\n",
    "        feat = np.concatenate([lineVec.flatten(),labVec])\n",
    "        if label_set['canonical_vendor_name'][i]==train_set['canonical_line_item_name'][j]:\n",
    "            y = 1\n",
    "            curr_y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "        X.append(feat)\n",
    "        Y.append(y)\n",
    "        # add the true lab if none of the similar indices is true label\n",
    "    if curr_y==0:\n",
    "        not_in_five.append(j)\n",
    "        trueLabId = label2idx[train_set['canonical_line_item_name'][j]]\n",
    "        trueLab = lab_mat[trueLabId,:]\n",
    "        feat = np.concatenate([lineVec.flatten(),trueLab])\n",
    "        X.append(feat)\n",
    "        Y.append(1)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1: 0.463\n"
     ]
    }
   ],
   "source": [
    "# Maybe RF is preferable compared to SVM--too slow to train\n",
    "# with different class weights\n",
    "\n",
    "weights = {0:0.05, 1:1.0}\n",
    "rf = RandomForestClassifier(n_estimators=1000, class_weight=weights,n_jobs=-1)\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(rf, X, Y, scoring='f1', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean F1: %.3f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F1 score doesn't look great. However, since the majority (~60% or more) of the instances are expected to be\n",
    "# classified with high accuracy, then the rest shouldn't hurt that much. Also given the limited possibilities given\n",
    "# the name of vendor, it shouldn't add too much burden for manually fixing the errors, or using some heuristics on\n",
    "# top of the classification result.\n",
    "# then fit the model with full dataset\n",
    "rf.fit(X,Y)\n",
    "\n",
    "# For the eval set, first do the same thing as the training set\n",
    "# Then select label from the possible label set with the trained RF.\n",
    "\n",
    "threshold = 0.4\n",
    "for_rf = []\n",
    "y_hat = []\n",
    "nrows = len(test_set.index)\n",
    "for j in range(nrows):\n",
    "    lineItem = re.sub('[^\\w\\s]','',test_set['line_item_name'][j]).lower().split()\n",
    "    lineDescrpt = re.sub('[^\\w\\s]','',test_set['line_item_name'][j]).lower().split()\n",
    "    lineVec = sparse.csr_matrix(line2vec(line,vocab=item_words,description=lineDescrpt))\n",
    "    vendor = test_set['canonical_vendor_name'][j]\n",
    "    possible_labs = vendor2item[vendor]\n",
    "    labIds = [lab2idx[lab] for lab in possible_labs]\n",
    "    subMat2matId = {i:lab2idx[lab] for i,lab in enumerate(possible_labs)}\n",
    "    subMat = lab_mat[labIds,:]\n",
    "    maxVal, maxIdx = cosinVal(lineVec,subMat)\n",
    "    if maxVal<threshold:\n",
    "        for_rf.append((j,[subMat2matId[k] for k in maxIdx]))\n",
    "    else:\n",
    "        y_hat.append((j,subMat2matId[maxIdx[0]]))\n",
    "\n",
    "#construct the test sets\n",
    "X_test = []\n",
    "labidlist = []\n",
    "xtest2labid = {}\n",
    "for j,ids in for_rf:\n",
    "    vendorVec = np.zeros(len(vendor2idx))\n",
    "    trueLab = train_set['canonical_line_item_name'][j]\n",
    "    lineItem = re.sub('[^\\w\\s]','',train_set['line_item_name'][j]).lower().split()\n",
    "    lineDescrpt = re.sub('[^\\w\\s]','',train_set['line_item_name'][j]).lower().split()\n",
    "    lineVec = line2vec(line,vocab=item_words,description=lineDescrpt)\n",
    "    for i in ids:\n",
    "        labidlist.append((j,i))\n",
    "        labVec = lab_mat[i,:]\n",
    "        feat = np.concatenate([lineVec.flatten(),labVec])\n",
    "        X_test.append(feat)\n",
    "\n",
    "for j,(r,idx) in enumerate(labidlist):\n",
    "    xtest2labid[j] = (r,idx)\n",
    "    \n",
    "Y_pred = rf.predict(X_test)\n",
    "for i,h in enumerate(Y_pred):\n",
    "    if h==1:\n",
    "        r,idx = xtest2labid[i]\n",
    "        y_hat.append((r,idx))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last step: deal with two possible prediction errors:\n",
    "# 1. one line item assigned to multiple canonical line items --> pick the one with highest cosine\n",
    "# 2. one line item assigned to none of the canonical line items --> pick the one with highest cosine\n",
    "\n",
    "predicted = list(sorted(y_hat))\n",
    "predDict = defaultdict(list)\n",
    "\n",
    "for j,h in predicted:\n",
    "    predDict[j].append(h)\n",
    "    \n",
    "out_lab_list = []\n",
    "    \n",
    "for i in range(len(test_set.index)):\n",
    "    if len(predDict[i]) == 0:\n",
    "        vendor = test_set['canonical_vendor_name'][i]\n",
    "        possible_labs = vendor2item[vendor]\n",
    "        labIds = [lab2idx[lab] for lab in possible_labs]\n",
    "        subMat2matId = {i:lab2idx[lab] for i,lab in enumerate(possible_labs)}\n",
    "        subMat = lab_mat[labIds,:]\n",
    "        maxVal, maxIdx = cosinVal(lineVec,subMat)\n",
    "        out_lab_list.append(subMat2matId[maxIdx[0]])\n",
    "        \n",
    "    elif len(predDict[i])>1:\n",
    "        cos = 0\n",
    "        arg = predDict[i][0]\n",
    "        lineItem = re.sub('[^\\w\\s]','',train_set['line_item_name'][i]).lower().split()\n",
    "        lineDescrpt = re.sub('[^\\w\\s]','',train_set['line_item_name'][i]).lower().split()\n",
    "        lineVec = line2vec(line,vocab=item_words,description=lineDescrpt)\n",
    "        for idx in predDict[i]:\n",
    "            labvec = lab_mat[idx]\n",
    "            sim = cosine_similarity(lineVec,labvec.reshape(1,labvec.shape[0]))\n",
    "            if sim>cos:\n",
    "                cos = sim\n",
    "                arg = idx\n",
    "        out_lab_list.append(arg)\n",
    "    else:\n",
    "        out_lab_list.append(predDict[i][0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to test_set\n",
    "\n",
    "out_labs = [idx2label[i] for i in out_lab_list]\n",
    "test_set['canonical_line_item_name'] = pd.Series(out_labs)\n",
    "test_set.to_csv(\"results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
